[[spring-cloud-stream-reference]]
= Spring Cloud Stream Reference Manual

[partintro]
--
This section goes into more detail about how you can work with Spring Cloud Stream. It covers topics
such as creating and running stream applications.
--

== Introducing Spring Cloud Stream

Spring Cloud Stream allows developing and running messaging microservices easily using Spring Integration, by just adding `@EnableBinding` and running your app as a Spring Boot application. Spring Cloud Stream applications connect to the physical broker through bindings, which link Spring Integration channels to physical broker destinations, or either input (consumer bindings) or output (producer bindings). The creation of the bindings, and therefore their broker-specific implementation is handled by a binder, which isanother important abstraction of Spring Cloud Stream. Binders abstract out the broker-specific implementation details. In order to connect to a specific type of broker (e.g. Rabbit or Kafka) you just need to have the relevant binder implementation on the classpath.

Here's a sample source application, with an output channel only:

[source,java]
----
@SpringBootApplication
public class StreamApplication {

  public static void main(String[] args) {
    SpringApplication.run(StreamApplication.class, args);
  }
}

@EnableBinding(Source.class)
public class TimerSource {

  @Value("${format}")
  private String format;

  @Bean
  @InboundChannelAdapter(value = Source.OUTPUT, poller = @Poller(fixedDelay = "${fixedDelay}", maxMessagesPerPoll = "1"))
  public MessageSource<String> timerMessageSource() {
    return () -> new GenericMessage<>(new SimpleDateFormat(format).format(new Date()));
  }
}
----

`@EnableBinding` is parameterized by one or more interfaces (in this case a single `Source` interface), which declares
input and/or output channels. The interfaces `Source`, `Sink` and `Processor` are provided off the shelf, but you can
define others. Here's the definition of `Source`:

[source,java]
----
public interface Source {
  String OUTPUT = "output";

  @Output(Source.OUTPUT)
  MessageChannel output();
}
----

The `@Output` annotation is used to identify output channels (messages leaving the app), and `@Input` is used to identify input channels (messages entering the app). It is optionally parameterized by a channel name - if the name is not provided the method name is used instead. An implementation of the interface is created for you and can be used in the application context by autowiring it, e.g. into a test case:

[source,java]
----
@RunWith(SpringJUnit4ClassRunner.class)
@SpringApplicationConfiguration(classes = StreamApplication.class)
@WebAppConfiguration
@DirtiesContext
public class StreamApplicationTests {

  @Autowired
  private Source source

  @Test
  public void contextLoads() {
    assertNotNull(this.source.output());
  }
}
----

== Spring Cloud Stream Main Concepts

Spring Cloud Stream provides a number of abstractions and primitives that simplify writing message-driven microservices. In this section we will provide an overview of:

* Spring Cloud Stream application model together with the Binder abstraction;
* Persistent publish-subscribe and consumer group support;
* Partitioning;
* Pluggable Binder API.


=== Application structure

A Spring Cloud Stream application consists of a middleware-neutral core that uses a Spring Messaging-based API and Spring Integration components, and communicates with the outside world through input and output channels that are managed and injected into it by the framework. A Spring Cloud Stream abstraction named a binder provides the bridging between the channels and the external brokers. Different binder implementations exist for different types of middleware, such as Kafka, Rabbit MQ, Redis or Gemfire, and an extensible API allows you to write your own binder.

.Spring Cloud Stream Application
image::SCSt-with-binder.png[width=300,scaledwidth="50%"]

Spring Cloud Stream uses Spring Boot for configuration, and the binder abstraction makes it possible for Spring Cloud Stream applications to be flexible in terms of how it connects to the middleware.

This can mean, for example, dynamically choosing the destinations that these channels connect to at runtime (e.g. Kafka topics or Rabbit MQ exchanges), via external configuration properties in any form that is supported by Spring Boot (application arguments, environment variables, `application.yml` files, etc). Taking the source example from the previous section, providing the `spring.cloud.stream.bindings.output.destination=raw-sensor-data` property to the application will cause it to write to the `raw-sensor-data` Kafka topic, or the `orders` exchange in Rabbit.

On the other hand, since the application is not middleware-specific, Spring Cloud Stream will automatically detect and use a binder that is found on the classpath, so you can easily use different types of middleware with the same code, just by including a different binder at build time. For more complex use cases, Spring Cloud Stream also provides the ability of packaging multiple binders within the same application and choosing what type of binder should be used at runtime, and even if multiple binders should be used at runtime for different channels.

=== Persistent publish subscribe and consumer groups

Spring Cloud Stream is a library focusing on building message-driven microservices, and more specifically stream processing applications. In such scenarios, communication between different logical applications follows a publish-subscribe pattern, with data being broadcast through shared topics. This can be seen in the following picture, which shows a typical deployment for a set of interacting Spring Cloud Stream applications.

.Spring Cloud Stream Application topologies
image::SCSt-with-binder.png[width=300,scaledwidth="50%"]

Data reported by sensors to an HTTP endpoint is sent to a common destination named `raw-sensor-data`, from where it is independently processed by a microservice that computes time windowed averages, as well as by a microservice that ingests the raw data into HDFS. In order to do so, both applications will declared the topic as their input at runtime. The fact that the communication is publish subscribe reduces the complexity of both the producer and the consumer, and allows adding new applications to the topology without disrupting the existing flow. For example, downstream from the average calculator we can have a component that calculates the highest temperature values in order to display and monitor them. Later on, we can add an application that interprets the very same flow of averages for fault detection. The fact that all the communication is done through shared topics rather than point to point queues reduces the coupling between microservices.

While the concept of publish-subscribe messaging is not new, Spring Cloud Stream takes the extra step of making it an opinionated choice for its application model, and makes it easy for users to work with it across different platform by using the native support of the middleware. To go with it, two other concepts are central to Spring Cloud Stream: consumer groups and durability.


[[consumer groups]]
==== Consumer Groups
While the publish subscribe model ensures that it is easy to connect multiple application by sharing a topic, it is equally important to be able to scale up by creating multiple instances of a given application, which would find themselves in a competing consumer relationship with each other. Spring Cloud Stream models this behavior through the concept of a consumer group, which is similar to (and inspired by) the notion of consumer groups in Kafka. Each consumer binding can specify a group name such as `spring.cloud.stream.bindings.input.group=hdfsWrite` or `spring.cloud.stream.bindings.input.group=average` as shown in the picture. All groups that subscribe to a given destination will receive a copy of the published data, but, within a group, only one application will receive a given message. By default, which is the case when a group is not specified, Spring Cloud Stream assigns the application to an anonymous, independent, single-member consumer group. Otherwise said, if no consumer group is specified for a binding, it will be in a publish-subscribe relationship with all the other consumer groups.

.Spring Cloud Stream Consumer Groups
image::SCSt-groups.png[width=300,scaledwidth="50%"]

[[durability]]
==== Durability
The other concept that is related to the publish subscribe model of Spring Cloud Stream is durability. Again, part of the opinionated application model of Spring Cloud Stream, consumer group subscriptions are durable. This is to say that the binder implementation will ensure that group subscriptions are persistent and, once at least one subscription for a group has been created, that group will receive messages, even if they are sent while all the applications of the group were stopped. Anonymous subscriptions are non-durable by nature. For some binder implementations (e.g. Rabbit) it is possible to have non-durable group subscriptions.

In general, it is preferable to always specify a consumer group when binding an application to a given destination. When scaling up a Spring Cloud Stream application, a consumer group must be specified for each of its input bindings, in order to prevent its instances from receiving duplicate messages (unless that behavior is desired, which is a less common use case).

[[partitioning]]
=== Partitioning

Another native concept of Spring Cloud Stream is partitioning. Spring Cloud Stream provides support for partitioning data between multiple instances of a given application. In a partitioned scenario, one or more producer application instances will send data multiple consumer application instances, ensuring that data with common characteristics is processed by the same consumer instance. The physical communication medium (e.g. the broker topic) is viewed as structured into multiple partitions. This happens regardless of whether the broker type is naturally partitioned (e.g. Kafka) or not (e.g. Rabbit), Spring Cloud Stream provides a common abstraction for implementing partitioned processing use cases in a uniform fashion.

.Spring Cloud Stream Partitioning
image::SCSt-partitioning.png[width=300,scaledwidth="50%"]

Partitioning is a critical concept in stateful processing, where ensuring that all the related data is processed together is critical for either performance or consistency. For example, in the time-windowed average calculation example, it is important that measurements from the same sensor land in the same application instance.

Setting up a partitioned processing scenario requires configuring both the data producing and the data consuming end.

== Programming model

This section will describe the programming model of Spring Cloud Stream, which consists from a number of predefined annotations that can be used to declare bound inputs and output channels, as well as how to listen to them.

=== Declaring and binding channels

==== Triggering binding via `@EnableBinding`

A Spring application becomes a Spring Cloud Stream application when the `@EnableBinding` annotation is applied to one of its configuration classes. `@EnableBinding` itself is meta-annotated with `@Configuration`, and triggers the configuration of Spring Cloud Stream infrastructure as follows:

[source,java]
----
...
@Import(...)
@Configuration
@EnableIntegration
public @interface EnableBinding {
    ...
    Class<?>[] value() default {};
}
----

`@EnableBinding` can be parameterized with one or more interface classes, containing methods that represent bindable components (typically message channels).

NOTE: As of version 1.0, the only supported bindable component is Spring Messaging `MessageChannel` and its extensions `SubscribableChannel` and `PollableChannel`. It is intended for future versions to extend support to other types of components, using the same mechanism. In this documentation, we will continue to refer to channels.

==== `@Input` and `@Output`

A Spring Cloud Stream application can have an arbitrary number of input and output channels defined as `@Input` and `@Output` methods in an interface, as follows:
[source,java]
----
public interface Barista {

    @Input
    SubscribableChannel orders();

    @Output
    MessageChannel hotDrinks();

    @Output
    MessageChannel coldDrinks();
}
----

Using this interface as a parameter to `@EnableBinding`, as in the following example, will trigger the creation of three bound channels named `orders`, `hotDrinks` and `coldDrinks` respectively.

[source,java]
----
@EnableBinding(Barista.class)
public class CafeConfiguration {

   ...
}
----

===== Customizing channel names

Both @Input and @Output allow specifying a customized name for the channel, as follows:

[source,java]
----
public interface Barista {
    ...
    @Input("inboundOrders")
    SubscribableChannel orders();
}
----
In this case, the name of the bound channel being created will be `inboundOrders`.

===== `Source`, `Sink`, and `Processor`

For ease of addressing the most common use cases that involve either an input or an output channel, or both, out of the box Spring Cloud Stream provides three predefined interfaces.

`Source` can be used for applications that have a single outbound channel.

[source,java]
----
public interface Source {

	String OUTPUT = "output";

	@Output(Source.OUTPUT)
	MessageChannel output();

}
----

`Sink` can be used for applications that have a single inbound channel.

[source,java]
----
public interface Sink {

	String INPUT = "input";

	@Input(Sink.INPUT)
	SubscribableChannel input();

}
----

`Processor` can be used for applications that have both an inbound and an outbound channel.

[source,java]
----
public interface Processor extends Source, Sink {
}
----

There is no special handling for either of these interfaces in Spring Cloud Stream, besides of the fact that they are provided out of the box.

==== Accessing bound channels

===== Injecting the bound interfaces

For each of the bound interfaces, Spring Cloud Stream will generate a bean that implements it, and for which invoking an `@Input` or `@Output` annotated method will return the bound channel. For example, the bean in the following example will send a message on the output channel every time
its `hello` method is invoked, using the injected `Source` bean, and invoking `output()` to retrieve the target channel.

[source,java]
----
@Component
public class SendingBean {

    @Autowire
    private Source source;

    public void sayHello(String name) {
		     source.output().send(MessageBuilder.withPayload(body).build());
	  }
}
----

===== Injecting channels directly

Bound channels can be also injected directly. For example:

[source, java]
----
@Component
public class SendingBean {

    @Autowire
    private MessageChannel output;

    public void sayHello(String name) {
		     output.send(MessageBuilder.withPayload(body).build());
	  }
}
----

Note that if the name of the channel is customized on the declaring annotation,
that name should be used instead of the method name. Considering this declaration:

[source,java]
----
public interface CustomSource {
    ...
    @Output("customOutput")
    MessageChannel output();
}
----

The channel will be injected as follows:

[source, java]
----
@Component
public class SendingBean {
    @Autowire
    private MessageChannel customOutput;

    public void sayHello(String name) {
		     customOutput.send(MessageBuilder.withPayload(body).build());
	  }
}
----

==== Programming model

While accessing bound channels directly can be useful in certain circumstances,
Spring Cloud Stream provides two main ways of developing applications. On one hand,
its programming model is based on Spring Integration, as that is the foundation of
the framework. In addition to that, it provides a `@StreamListener` annotation of its own modeled by the other
similar Spring Messaging annotations (e.g. `@MessageMapping`, `@JmsListener`, `@RabbitListener`, etc.)
especially for dealing with use cases that involve transparent content type management
and type coercion.

===== Native Spring Integration support

Due to the fact that Spring Cloud Stream is Spring Integration based, it completely inherits
its foundation and infrastructure, as well as the component. For example, the output channel of
a `Source` can be attached to a `MessageSource`, as follows:

[source, java]
----
@EnableBinding(Source.class)
public class TimerSource {

  @Value("${format}")
  private String format;

  @Bean
  @InboundChannelAdapter(value = Source.OUTPUT, poller = @Poller(fixedDelay = "${fixedDelay}", maxMessagesPerPoll = "1"))
  public MessageSource<String> timerMessageSource() {
    return () -> new GenericMessage<>(new SimpleDateFormat(format).format(new Date()));
  }
}
----

Or, the channels of a processor can be used in a transformer, as follows:

[source,java]
----
@EnableBinding(Processor.class)
public class TransformProcessor {
	@Transformer(inputChannel = Processor.INPUT, outputChannel = Processor.OUTPUT)
	public Object transform(String message) {
		return message.toUpper();
	}
}
----

===== @StreamListener for automatic content type handling

Complementary to the Spring Integration support, Spring Cloud Stream provides a
`@StreamListener` annotation of its own modeled by the other similar Spring Messaging
annotations (e.g. `@MessageMapping`, `@JmsListener`, `@RabbitListener`, etc.)
especially for dealing with use cases that involve content type management
and type coercion.

This approach is suited for applications that process inbound data that specifies content types, as Spring Cloud Stream provides an extensible `MessageConverter` registration mechanism for handling data conversion by the bound channels and for dispatching to `@StreamListener` annotated methods.

For example, an application that processes external `Vote` events can be declared as follows:

[source,java]
----
@EnableBinding(Sink.class)
public class VoteHandler {

  @Autowired
  VotingService votingService;

  @StreamListener(Sink.INPUT)
	public void handle(Vote vote) {
		votingService.record(vote);
	}
}
----

The fundamental distinction between this approach and a Spring Integration `@ServiceActivator` becomes relevant if one considers an inbound `Message` with a `String` payload and a `contentType` header of `application/json`. For `@StreamListener`, the `MessageConverter` mechanism will use the `contentType` header to parse the `String` into a `Vote` object.

Just as with the other Spring Messaging methods, method arguments can be annotated with `@Payload`, `@Headers` and `@Header`. For methods that return data, `@SendTo` must be used for specifying the output binding destination for data returned by the methods as follows.

[source,java]
----
@EnableBinding(Processor.class)
public class TransformProcessor {

  @Autowired
  VotingService votingService;

  @StreamListener(Processor.INPUT)
  @SendTo(Processor.OUTPUT)
	public VoteResult handle(Vote vote) {
		return votingService.record(vote);
	}
}
----

NOTE: Content type headers can be either set by external applications in the case of Rabbit MQ, and they are supported as part of an extended internal protocol by Spring Cloud Stream for any type of transport (even the ones that do not support headers normally, like Kafka).

== Configuration options reference

The way Spring Cloud Stream behaves can be configured in two main ways:

Configuration options can be provided per-binding. They include: Spring Cloud Stream-specific properties that control the general behavior of the binding, as well as consumer and producer properties for connecting to the middleware. A number of common consumer and producer properties are available to all binders, and some binders can specify additional properties that they support.

In addition, each binder allows some binder-specific properties that configure the general behavior of the binder (e.g. hosts to connect to, connectivity parameters, etc). These properties are distinct from the binding properties.

In both cases, properties are provided to Spring Cloud Stream through all the mechanisms supported by Spring Boot: application arguments, environment variables, YML files etc.

=== Binding properties

Binding properties are supplied using the format `spring.cloud.stream.bindings.<channelName>.<property>=<value>`. `<channelName>` represents the name of the channel being configured, e.g. `output` for a `Source`. In what follows, we will indicate where the `spring.cloud.stream.bindings.<channelName>.` prefix is omitted and focus just on the property name, with the understanding that the prefix will be included at runtime.

==== Spring Cloud Stream Properties

spring.cloud.stream.instanceCount::
  The number of deployed instances of the same application. Must be set
  for partitioning and with Kafka. Default value is `1`.
spring.cloud.stream.instanceIndex::
  The instance index of the application, a number from `0` to `instanceCount`-1.
  Used for partitioning and with Kafka. Automatically set in Cloud Foundry to
  match the instance index of the application.

==== Properties for the use of Spring Cloud Stream

The following binding properties are available for both input and output bindings and
must be prefixed with `spring.cloud.stream.bindings.<channelName>.` .

destination::
    The target destination of channel on the bound middleware, e.g. Rabbit exchange or
    Kafka topic. If not set, the channel name will be used instead.
group::
    The consumer group of the channel. This property applies only to inbound bindings.
    By default it is null, and indicates an anonymous consumer. See <<consumer groups>>.
contentType::
    The content type of the channel. By default it is `null` and no type
    coercion is performed. See <<content type management>>.
binder::
    The binder used by this binding. By default, it is set to `null` and will
    use the default binder, if one exists. See <<multiple binders>> for details.

==== Consumer properties

The following binding properties are available for input bindings only and
must be prefixed with `spring.cloud.stream.bindings.<channelName>.`:

concurrency::
  The concurrency of the inbound consumer. By default, set to `1`.
partitioned::
  Must be set to `true` if the consumer is receiving data from a partitioned
  producer. By default it is set to `false`.
maxAttempts::
  The number of attempts of re-processing an inbound message. Default '3'. (Ignored by Kafka, currently).
backOffInitialInterval::
  The backoff initial interval on retry. Default `1000`.(Ignored by Kafka, currently).
backOffMaxInterval::
  The maximum backoff interval. Default `10000`.(Ignored by Kafka, currently).
backOffMultiplier::
  The backoff multiplier. Default `2.0`.

==== Producer properties

The following binding properties are available for output bindings only and
must be prefixed with `spring.cloud.stream.bindings.<channelName>.`:

partitionKeyExpression::
  A SpEL expression for partitioning outbound data. Default: `null`. If either this is set or
  `partitionKeyExtractorClass` is present, outbound data on this channel will be partitioned,
  and `partitionCount` must be set to a value larger than 1 to be effective.
  The two options are mutually exclusive. See <<partitioning>>.
partitionKeyExtractorClass::
  A `PartitionKeyExtractorStrategy` implementation. Default: `null`. If either this is set or
  `partitionKeyExpression` is present, outbound data on this channel will be partitioned,
  and `partitionCount` must be set to a value larger than 1 to be effective.
  The two options are mutually exclusive. See <<partitioning>>.
partitionSelectorClass::
  A `PartitionSelectorStrategy` implementation. Default `null`. Mutually exclusive with
  `partitionSelectorExpression`. If none is set, the partition will be selected as the
  `hashCode(key) % partitionCount`, where `key` is computed via either `partitionKeyExpression`
  or `partitionKeyExtractorClass`.
partitionSelectorExpression::
  A SpEL expression for customizing partition selection. Default `null`. Mutually exclusive with
  `partitionSelectorClass`. If none is set, the partition will be selected as the
  `hashCode(key) % partitionCount`, where `key` is computed via either `partitionKeyExpression`
  or `partitionKeyExtractorClass`.
partitionCount::
  The number of target partitions for the data, if partitioning is enabled. Default `1`. Must be
  set to a value higher than `1` if the producer is partitioned. On Kafka it is interpreted as a
  hint, and the larger of this and the partition count of the target topic will be used instead.
requiredGroups::
  A comma separated list of groups that the producer must ensure message delivery even if they
  start after it has been created (e.g. by pre-creating durable queues in Rabbit MQ).


== Binder-specific configuration

=== Rabbit Binder properties

The binder supports the all Spring Boot properties for Rabbit MQ configuration.

In addition to that, it also supports the following properties:

spring.cloud.stream.binder.rabbit.addresses::
  A comma-separated list of RabbitMQ server addresses (used only for clustering and in conjunction with `nodes`). Default empty.
spring.cloud.stream.binder.rabbit.adminAddresses. Default empty.
  A comma-separated list of RabbitMQ management plugin URLs - only used when nodes contains more than one entry. Entries in this list must correspond to the corresponding entry in addresses. Default empty.
spring.cloud.stream.binder.rabbit.nodes::
  A comma-separated list of RabbitMQ node names; when more than one entry, used to locate the server address where a queue is located. Entries in this list must correspond to the corresponding entry in addresses. Default empty.
spring.cloud.stream.binder.rabbit.username::
  The user name. Default `null`.
spring.cloud.stream.binder.rabbit.password::
  The password. Default `null`.
spring.cloud.stream.binder.rabbit.vhost::
  The virtual host. Default `null`.
spring.cloud.stream.binder.rabbit.useSSL::
  True if rabbit should use SSL.
spring.cloud.stream.binder.rabbit.sslPropertiesLocation::
  The location of the SSL properties file, when certificate exchange is used.
spring.cloud.stream.binder.rabbit.compressionLevel::
  Compression level for compressed bindings. Defaults to `1` (BEST_LEVEL). See `java.util.zip.Deflater`.

=== Rabbit Consumer Properties

The following properties are available for Rabbit consumers only and
must be prefixed with `spring.cloud.stream.bindings.<channelName>.`

acknowledgeMode::
  The acknowledge mode. Default `AUTO`.
autoBindDlq::
  Whether to automatically declare the DLQ and bind it to the binder DLX. Default `false`.
durableSubscription::
  Whether subscription should be durable. Only effective if `group` is also set. Default `true`.
maxConcurrency:
  Default `1`.
prefetch:
  Prefetch count. Default `1`.
prefix::
  A prefix to be added to the name of the `destination` and queues. Default "".
requeueRejected::
  Whether delivery failures should be requeued. Default `true`.
requestHeaderPatterns::
  The request headers to be transported. Default `[STANDARD_REQUEST_HEADERS,'*']`.
replyHeaderPatterns::
  The reply headers to be transported. Default `[STANDARD_REQUEST_HEADERS,'*']`
republishToDlq::
  By default, failed messages after retries are exhausted are rejected. If a dead-letter queue (DLQ) is configured, rabbitmq will route the failed message (unchanged) to the DLQ. Setting this property to true instructs the bus to republish failed messages to the DLQ, with additional headers, including the exception message and stack trace from the cause of the final failure.
transacted::
  Whether to use transacted channels. Default `false`.
txSize::
  The number of deliveries between acks. Default `1`.

=== Rabbit Producer Properties

The following properties are available for Rabbit producers only and
must be prefixed with `spring.cloud.stream.bindings.<channelName>.`

autoBindDlq::
  Whether to automatically declare the DLQ and bind it to the binder DLX. Default `false`.
batchingEnabled::
  True to enable message batching by producers. Default `false`.
batchSize::
  The number of message to buffer when batching is enabled. Default `100`.
batchBufferLimit::
  Default `10000`.
batchTimeout::
  Default `5000`.
compress::
  Whether data should be compressed when sent. Default `false`.
deliveryMode::
  Delivery mode. Default `PERSISTENT`.
prefix::
  A prefix to be added to the name of the `destination` and queues. Default "".
requestHeaderPatterns::
  The request headers to be transported. Default `[STANDARD_REQUEST_HEADERS,'*']`.
replyHeaderPatterns::
  The reply headers to be transported. Default `[STANDARD_REQUEST_HEADERS,'*']`



== Binder detection

Spring Cloud Stream relies on implementations of the Binder SPI to perform the task of connecting channels to message
brokers. Each Binder implementation typically connects to one type of messaging system. Spring Cloud Stream provides
out of the box binders for Kafka, RabbitMQ and Redis.

=== Classpath Detection

By default, Spring Cloud Stream relies on Spring Boot's auto-configuration to configure the binding process. If a
single binder implementation is found on the classpath, Spring Cloud Stream will use it automatically. So, for example,
a Spring Cloud Stream project that aims to bind only to RabbitMQ can simply add the following dependency:

[source,xml]
----
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-stream-binder-rabbit</artifactId>
</dependency>
----

<<multiple binders>>
=== Multiple Binders on the Classpath

When multiple binders are present on the classpath, the application must indicate which binder is to be used for each
channel binding. Each binder configuration contains a `META-INF/spring.binders`, which is a simple properties file:

[source]
----
rabbit:\
org.springframework.cloud.stream.binder.rabbit.config.RabbitServiceAutoConfiguration
----

Similar files exist for the other binder implementations (e.g. Kafka), and it is expected that custom binder
implementations will provide them, too. The key represents an identifying name for the binder implementation, whereas
the value is a comma-separated list of configuration classes that contain one and only one bean definition of the type
`org.springframework.cloud.stream.binder.Binder`.

Selecting the binder can be done globally by either using the `spring.cloud.stream.defaultBinder` property, e.g.
`spring.cloud.stream.defaultBinder=rabbit`, or by individually configuring them on each channel binding.

For instance, a processor app that reads from Kafka and writes to Rabbit can specify the following configuration:
`spring.cloud.stream.bindings.input.binder=kafka`,`spring.cloud.stream.bindings.output.binder=rabbit`.

=== Connecting to Multiple Systems

By default, binders share the Spring Boot auto-configuration of the application and create one instance of each binder
found on the classpath. In scenarios where an application should connect to more than one broker of the same type,
Spring Cloud Stream allows you to specify multiple binder configurations, with different environment settings. Please
note that turning on explicit binder configuration will disable the default binder configuration process altogether, so
all the binders in use must be included in the configuration.

For example, this is the typical configuration for a processor that connects to two RabbitMQ broker instances:

[source,yml]
----
spring:
  cloud:
    stream:
      bindings:
        input:
          destination: foo
          binder: rabbit1
        output:
          destination: bar
          binder: rabbit2
      binders:
        rabbit1:
          type: rabbit
          environment:
            spring:
              rabbitmq:
                host: <host1>
        rabbit2:
          type: rabbit
          environment:
            spring:
              rabbitmq:
                host: <host2>
----


== Binder SPI

== Miscellaneous


[[contenttypemanagement]]
=== Content Type and Headers

=== Inter-app Communication

While Spring Cloud Stream makes it easy for individual boot apps to connect to messaging systems, the typical scenario
for Spring Cloud Stream is the creation of multi-app pipelines, where microservice apps are sending data to each other.
This can be achieved by correlating the input and output destinations of adjacent apps, as in the following example.

Supposing that the design calls for the `time-source` app to send data to the `log-sink` app, we will use a
common destination named `ticktock` for bindings within both apps. `time-source` will set
`spring.cloud.stream.bindings.output.destination=ticktock`, and `log-sink` will set
`spring.cloud.stream.bindings.input.destination=ticktock`.


=== Instance Index and Instance Count

When scaling up Spring Cloud Stream applications, each instance can receive information about how many other instances
of the same application exist and what its own instance index is. This is done through the
`spring.cloud.stream.instanceCount` and `spring.cloud.stream.instanceIndex` properties. For example, if there are 3
instances of the HDFS sink application, all three will have `spring.cloud.stream.instanceCount` set to 3, and the
applications will have `spring.cloud.stream.instanceIndex` set to 0, 1 and 2, respectively. When Spring Cloud Stream
applications are deployed via Spring Cloud Data Flow, these properties are configured automatically, but when Spring
Cloud Stream applications are launched independently, these properties must be set correctly. By default
`spring.cloud.stream.instanceCount` is 1, and `spring.cloud.stream.instanceIndex` is 0.

Setting up the two properties correctly on scale up scenarios is important for addressing partitioning behavior in
general (see below), and they are always required by certain types of binders (e.g. the Kafka binder) in order to
ensure that data is split correctly across multiple consumer instances.

===== Partitioning


====== Configuring Output Bindings for Partitioning

An output binding is configured to send partitioned data, by setting one and only one of its `partitionKeyExpression`
or `partitionKeyExtractorClass` properties, as well as its `partitionCount` property. For example, setting
`spring.cloud.stream.bindings.output.partitionKeyExpression=payload.id`,`spring.cloud.stream.bindings.output.partitionCount=5`
is a valid and typical configuration.

Based on this configuration, the data will be sent to the target partition using the following logic. A partition key's
value is calculated for each message sent to a partitioned output channel based on the `partitionKeyExpression`. The
`partitionKeyExpression` is a SpEL expression that is evaluated against the outbound message for extracting the
partitioning key. If a SpEL expression is not sufficient for your needs, you can instead calculate the partition key
value by setting the property `partitionKeyExtractorClass`. This class must implement the interface
`org.springframework.cloud.stream.binder.PartitionKeyExtractorStrategy`. While, in general, the SpEL expression should
suffice, more complex cases may use the custom implementation strategy.

Once the message key is calculated, the partition selection process will determine the target partition as a value
between `0` and `partitionCount - 1`. The default calculation, applicable in most scenarios is based on the formula
`key.hashCode() % partitionCount`. This can be customized on the binding, either by setting a SpEL expression to be
evaluated against the key via the `partitionSelectorExpression` property, or by setting a
`org.springframework.cloud.stream.binder.PartitionSelectorStrategy` implementation via the `partitionSelectorClass`
property.

Additional properties can be configured for more advanced scenarios, as described in the following section.

====== Configuring Input Bindings for Partitioning

An input binding is configured to receive partitioned data by setting its `partitioned` property, as well as the
instance index and instance count properties on the app itself, as follows:
`spring.cloud.stream.bindings.input.partitioned=true`,`spring.cloud.stream.instanceIndex=3`,`spring.cloud.stream.instanceCount=5`.
The instance count value represents the total number of app instances between which the data needs to be partitioned,
whereas instance index must be a unique value across the multiple instances, between `0` and `instanceCount - 1`. The
instance index helps each app instance to identify the unique partition (or in the case of Kafka, the partition set)
from which it receives data. It is important that both values are set correctly in order to ensure that all the data is
consumed, and that the app instances receive mutually exclusive datasets.

While setting up multiple instances for partitioned data processing may be complex in the standalone case, Spring Cloud
Data Flow can simplify the process significantly, by populating both the input and output values correctly, as well as
relying on the runtime infrastructure to provide information about the instance index and instance count.


=== Managed vs Standalone

Code using the Spring Cloud Stream library can be deployed as a standalone application or be used as a Spring Cloud
Data Flow module. In standalone mode, your application will run happily as a service or in any PaaS (Cloud Foundry,
Heroku, Azure, etc.). Spring Cloud Data Flow helps orchestrate the communication between instances, so the aspects of
configuration that deal with application interconnection will be configured transparently.

==== Fat JAR

You can run in standalone mode from your IDE for testing. To run in production you can create an executable (or "fat")
JAR using the standard Spring Boot tooling provided for Maven or Gradle.

==== Health Indicator

Spring Cloud Stream provides a health indicator for the binders, registered under the name of `binders`. It can be
enabled or disabled using the `management.health.binders.enabled` property.

=== Binder SPI

As described above, Spring Cloud Stream provides a binder abstraction for connecting to physical destinations. This
section will provide more information about the main concepts behind the Binder SPI, its main components, as well as
details specific to different implementations.

==== Producers and Consumers

.Producers and Consumers
image::producers-consumers.png[width=300,scaledwidth="75%"]

A producer is any component that sends messages to a channel. That channel can be bound to an external message broker
via a `Binder` implementation for that broker. When invoking the `bindProducer` method, the first parameter is the name
of the destination within that broker. The second parameter is the local channel instance to which the producer will be
sending messages, and the third parameter contains properties to be used within the adapter that is created for that
channel, such as a partition key expression.

A consumer is any component that receives messages from a channel. As with the producer, the consumer’s channel can be
bound to an external message broker, and the first parameter for the `bindConsumer` method is the destination name.
However, on the consumer side, a second parameter provides the name of a logical group of consumers. Each group
represented by consumer bindings for a given destination will receive a copy of each message that a producer sends to
that destination (i.e. pub/sub semantics). If there are multiple consumer instances bound using the same group name,
then messages will be load balanced across those consumer instances so that each message sent by a producer would only
be consumed by a single consumer instance within each group (i.e. queue semantics).

==== Kafka Binder

.Kafka Binder
image::kafka-binder.png[width=300,scaledwidth="50%"]

The Kafka Binder implementation maps the destination to a Kafka topic, and the consumer group maps directly to the same
Kafka concept. Spring Cloud Stream does not use the high level consumer, but implements a similar concept for the
simple consumer.

==== RabbitMQ Binder

.RabbitMQ Binder
image::rabbit-binder.png[width=300,scaledwidth="50%"]

The RabbitMQ Binder implementation maps the destination to a `TopicExchange`, and for each consumer group, a `Queue`
will be bound to that `TopicExchange`. Each consumer instance that binds will trigger creation of a corresponding
RabbitMQ `Consumer` instance for its group’s `Queue`.

==== Redis Binder

.Redis Binder
image::redis-binder.png[width=300,scaledwidth="50%"]

NOTE: we recommend only using the Redis Binder for development

The Redis Binder creates a `LIST` (which performs the role of a queue) for each consumer group. A consumer binding will
trigger `BRPOP` operations on its group's `LIST`. A producer binding will consult a `ZSET` to determine what groups
currently have active consumers, and then for each message being sent, an `LPUSH` operation will be executed on each of
those group's `LISTs`.

=== Samples

For Spring Cloud Stream samples, please refer: https://github.com/spring-cloud/spring-cloud-stream-samples
